# AI Agent 降本增效改进方案

**分析日期:** 2025年9月24日
**分析基于:** `logs/session_20250924_165510/165618_LLM6.log` 及相关用量统计

## 1. 现状分析

根据提供的用量统计，我们观察到当前的 AI Agent 执行一次任务（从Python代码生成到PikaPython C模块并验证）的总费用约为 **0.035元**，耗时 **1分8秒**。费用和Token消耗分布如下：

- **缓存输入 (Cache Input):** 15.30k tokens, 占总费用的 **21.56%**。
- **增量输入 (Fresh Input):** 3.87k tokens, 占总费用的 **43.59%**。
- **模型输出 (Output):** 1.03k tokens, 占总费用的 **34.85%**。

从日志 `165618_LLM6.log` 中可以看到，Agent 的核心交互流程包括：代码生成 -> 测试脚本生成 -> 构建运行 -> 结果读取 -> 总结。在每一次与 LLM 的交互中，系统都传入了非常详细的、篇幅很长的 `core_task.md` 作为系统指令（System Prompt）。

**核心问题：**
1.  **冗余的上下文传输：** `core_task.md` 在每次请求中都重复发送，虽然大部分被缓存（Cache Input），但仍占据了显著的成本，并且增加了服务端的处理负担。
2.  **过量的增量输入：** 将工具（如 `run_shell`、`read_file`）的完整输出作为新的输入（Fresh Input）反馈给 LLM，导致成本高昂。例如，将完整的运行日志文件内容全部发给模型进行总结。
3.  **不必要的最终总结步骤：** 在 `run_shell` 执行成功并返回了清晰的 `run.log` 摘要后，Agent 仍然多进行了一次 `read_file` 和一次 LLM 调用来生成最终的 `[MODULE]` 和 `[SUMMARY]` 块。这个步骤是多余的，因为所需信息在 `run_shell` 的输出中已经完全可用。

## 2. 改进方案

为了显著降低费用和执行时间，我们提出以下三点核心改进策略：

### 方案一：引入“阶段化提示词” (Staged Prompts)

**目标：** 大幅减少缓存输入（Cache Input）和增量输入（Fresh Input）的 Token 消耗。

**措施：**
放弃在每次请求中都发送完整的 `core_task.md`。取而代之，根据任务所处的不同阶段，使用更简洁、更有针对性的提示词。

- **阶段 1 (代码生成):** 保持详细的指令，因为此阶段需要精确的规则来生成代码。
- **阶段 2 (构建与运行):** 当文件生成完毕后，切换到一个极简的提示词，例如：“文件已生成。现在执行构建和运行命令。”
- **阶段 3 (结果提取与总结):** 在命令执行后，提示词应为：“构建已完成。请从以下工具输出中提取关键性能指标和自测结果。”

**预期效果：**
- 缓存输入 Token 预计可减少 **80%** 以上。
- 减少 LLM 的上下文理解负担，可能缩短响应时间。

### 方案二：精简工具输出作为LLM输入

**目标：** 降低增量输入（Fresh Input）的成本。

**措施：**
在客户端（`Client.py`）或工具层面对工具的输出进行预处理，只将最关键的信息反馈给 LLM。

- **`run_shell` 工具:** `run_pika.py` 脚本已经很优秀地提供了日志的尾部摘要。我们应强制 Agent **仅使用这个摘要**，而不是在 `run_shell` 成功后再次调用 `read_file` 去读取完整的 `run.log`。
- **`read_file` 工具:** 如果必须读取文件，应指导 Agent 优先读取文件的末尾部分（tail），因为日志和错误信息通常出现在最后。

**预期效果：**
- 增量输入 Token 预计可减少 **50%**。
- 显著降低单次任务的费用。

### 方案三：优化 Agent 状态机，减少LLM调用次数

**目标：** 消除不必要的 LLM 交互，直接在客户端完成任务。

**措施：**
在 `Client.py` 中调整 Agent 的逻辑，使其在获取到足够信息后能直接终止并自行格式化输出，而无需再次请求 LLM。

- **识别任务完成信号：** 当 `run_shell` 命令成功执行，并且其标准输出中包含 `[EXAMPLE][SELFTEST]` 和 `OK` 标志时，即可判定任务成功。
- **客户端直接合成最终结果：** 任务成功后，客户端应立即捕获 `run_shell` 输出中的关键性能行，并自行组装成最终的 `[MODULE]` 和 `[SUMMARY]` 报告，然后终止流程。这可以完全省去最后两次工具调用（`read_file`）和一次 LLM 调用。

**预期效果：**
- 每次任务减少 **1-2 次** LLM 调用。
- 任务总执行时间预计可缩短 **15-25秒**。
- 进一步降低模型输出（Output）和增量输入（Fresh Input）的成本。

## 3. 总结与预估收益

通过实施以上三项改进，我们预期可以达到以下效果：

- **总费用降低：** 预计可将单次任务成本从 0.035 元降低到 **0.01元以下**，降幅超过 **70%**。
- **总耗时减少：** 预计可将执行时间从 1分8秒 缩短到 **45秒以内**，提速约 **35%**。

建议优先实施 **方案三** 和 **方案二**，因为它们能最直接地减少工具调用和数据传输量，实现立竿见影的降本效果。随后，实施 **方案一** 以进一步优化长期成本。